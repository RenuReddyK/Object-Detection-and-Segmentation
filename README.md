# Object-Detection-and-Segmentation

## YOLO:
Implemented from scratch based on the original research paper, YOLO (You Only Look Once) is a real-time object detection algorithm that efficiently processes images in a single pass, providing high accuracy and speed.
<p align="center">

</p>

## SOLO:
Developed from the ground up using research paper insights, SOLO (Segmentation of Objects by Learning Only) is a sophisticated instance segmentation model that excels in precisely delineating individual object instances within an image.
<p align="center">

</p>

## SegFormer:
Constructed from the research paper, SegFormer is a cutting-edge semantic segmentation model that adopts a transformer-based architecture, demonstrating state-of-the-art performance in pixel-level image segmentation tasks.
<p align="center">

</p>

## VAE:
Implemented a Variational Autoencoder (VAE) on the Fashion MNIST dataset involving training a neural network architecture to learn a probabilistic mapping of input images to a latent space. 
<p align="center">

</p>

## CycleGAN:
Constructed from scratch following the research paper, CycleGAN on STL-10 dataset. It is a powerful unsupervised learning model that facilitates domain adaptation by mapping images from one domain to another without paired training data, showcasing impressive results in image-to-image translation tasks.
<p align="center">

</p>

# References:
[1] https://doi.org/10.48550/arXiv.1506.02640 <br>
[2] https://doi.org/10.48550/arXiv.1912.04488 <br>
[3] https://doi.org/10.48550/arXiv.2105.15203 <br>
[4] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. <br>
[5] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information pro- cessing systems, pages 2672–2680, 2014. <br>
[6] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223–2232, 2017.
